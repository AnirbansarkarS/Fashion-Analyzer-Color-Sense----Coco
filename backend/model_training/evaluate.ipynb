{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Add backend to path\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[38;5;28mstr\u001b[39m(Path(\u001b[34;43m__file__\u001b[39;49m).parent))\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FashionModel\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_training\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_preprocess\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FashionDatasetPreprocessor\n",
      "\u001b[31mNameError\u001b[39m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model Evaluation Script\n",
    "Evaluate model performance and generate metrics\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add backend to path\n",
    "sys.path.insert(0, str(Path(__file__).parent))\n",
    "\n",
    "from model_training.train_model import FashionModel\n",
    "from model_training.dataset_preprocess import FashionDatasetPreprocessor\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Evaluate fashion model performance\"\"\"\n",
    "    \n",
    "    def __init__(self, model=None):\n",
    "        self.model = model\n",
    "    \n",
    "    def evaluate_on_test_set(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate model on test dataset\n",
    "        \"\"\"\n",
    "        if self.model is None or self.model.model is None:\n",
    "            print(\"‚úó Model not loaded\")\n",
    "            return None\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = []\n",
    "        for img in X_test:\n",
    "            pred = self.model.predict(np.expand_dims(img, axis=0))\n",
    "            predictions.append(pred['style_match'] * 10)\n",
    "        \n",
    "        predictions = np.array(predictions).reshape(-1, 1)\n",
    "        y_true = y_test * 10\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_true, predictions)\n",
    "        mae = mean_absolute_error(y_true, predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, predictions)\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üìä MODEL EVALUATION RESULTS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mean Squared Error (MSE):  {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error:   {rmse:.4f}\")\n",
    "        print(f\"Mean Absolute Error:       {mae:.4f}\")\n",
    "        print(f\"R¬≤ Score:                  {r2:.4f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return metrics, predictions\n",
    "    \n",
    "    def plot_predictions(self, y_true, y_pred, save_path='evaluation/predictions_plot.png'):\n",
    "        \"\"\"Plot predicted vs actual scores\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.scatter(y_true, y_pred, alpha=0.6, s=50)\n",
    "        plt.plot([0, 10], [0, 10], 'r--', lw=2, label='Perfect prediction')\n",
    "        \n",
    "        plt.xlabel('True Fashion Score', fontsize=12)\n",
    "        plt.ylabel('Predicted Fashion Score', fontsize=12)\n",
    "        plt.title('Fashion Score Predictions vs Ground Truth', fontsize=14, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xlim(0, 10)\n",
    "        plt.ylim(0, 10)\n",
    "        \n",
    "        # Save\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Plot saved to {save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_training_history(self, history, save_path='evaluation/training_history.png'):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if history is None:\n",
    "            print(\"‚ö† No training history available\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[0].set_ylabel('Loss', fontsize=11)\n",
    "        axes[0].set_title('Model Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # MAE plot\n",
    "        axes[1].plot(history.history['mae'], label='Train MAE', linewidth=2)\n",
    "        axes[1].plot(history.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[1].set_ylabel('MAE', fontsize=11)\n",
    "        axes[1].set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Save\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Training history plot saved to {save_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    def generate_confusion_matrix(self, y_true, y_pred, bins=5):\n",
    "        \"\"\"Generate confusion matrix for binned scores\"\"\"\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        \n",
    "        # Bin scores into categories\n",
    "        y_true_binned = np.digitize(y_true, bins=np.linspace(0, 10, bins+1)) - 1\n",
    "        y_pred_binned = np.digitize(y_pred, bins=np.linspace(0, 10, bins+1)) - 1\n",
    "        \n",
    "        cm = confusion_matrix(y_true_binned, y_pred_binned)\n",
    "        \n",
    "        # Plot\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "        plt.xlabel('Predicted Category', fontsize=11)\n",
    "        plt.ylabel('True Category', fontsize=11)\n",
    "        plt.title('Confusion Matrix (Score Categories)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        import os\n",
    "        os.makedirs('evaluation', exist_ok=True)\n",
    "        plt.savefig('evaluation/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Confusion matrix saved to evaluation/confusion_matrix.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    def get_error_analysis(self, y_true, y_pred):\n",
    "        \"\"\"Analyze prediction errors\"\"\"\n",
    "        errors = np.abs(y_true - y_pred).flatten()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üîç ERROR ANALYSIS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Mean Error:                {np.mean(errors):.4f}\")\n",
    "        print(f\"Median Error:              {np.median(errors):.4f}\")\n",
    "        print(f\"Std Dev of Errors:         {np.std(errors):.4f}\")\n",
    "        print(f\"Min Error:                 {np.min(errors):.4f}\")\n",
    "        print(f\"Max Error:                 {np.max(errors):.4f}\")\n",
    "        print(f\"% Predictions within ¬±1:   {100*np.sum(errors <= 1) / len(errors):.2f}%\")\n",
    "        print(f\"% Predictions within ¬±0.5: {100*np.sum(errors <= 0.5) / len(errors):.2f}%\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return {\n",
    "            'mean_error': np.mean(errors),\n",
    "            'median_error': np.median(errors),\n",
    "            'std_error': np.std(errors),\n",
    "            'max_error': np.max(errors)\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Model Evaluation...\\n\")\n",
    "    \n",
    "    # Load dataset\n",
    "    processor = FashionDatasetPreprocessor()\n",
    "    dataset = processor.prepare_fashion_mnist()\n",
    "    \n",
    "    # Load model\n",
    "    model = FashionModel()\n",
    "    model.load_model(\"models/fashion_model.h5\")\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluator = ModelEvaluator(model)\n",
    "    metrics, predictions = evaluator.evaluate_on_test_set(\n",
    "        dataset['X_test'], \n",
    "        dataset['y_test']\n",
    "    )\n",
    "    \n",
    "    # Analysis\n",
    "    error_stats = evaluator.get_error_analysis(\n",
    "        dataset['y_test'] * 10,\n",
    "        predictions\n",
    "    )\n",
    "    \n",
    "    # Plots\n",
    "    if model.history:\n",
    "        evaluator.plot_training_history(model.history)\n",
    "    \n",
    "    evaluator.plot_predictions(dataset['y_test'] * 10, predictions)\n",
    "    evaluator.generate_confusion_matrix(dataset['y_test'] * 10, predictions)\n",
    "    \n",
    "    print(\"‚úì Evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
